{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\envs\\torch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class View(nn.Module):\n",
    "    def __init__(self,shape):\n",
    "        super().__init__()\n",
    "        self.shape = (shape,)\n",
    "    def forward(self,x):\n",
    "        return x.view(*self.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Name(nn.Module):\n",
    "    def __init__(self,resl):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self,x): \n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mySequential(nn.Sequential):\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self,*x): \n",
    "        for layer in self.children():\n",
    "            if type(x) is tuple:\n",
    "                x = layer(*x)\n",
    "            else:\n",
    "                x= layer(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## equal LR 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class EqualLR:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def compute_weight(self,module):\n",
    "        # num_fanin = \n",
    "        weight = getattr(module,'weight_orig1')\n",
    "        fan_in = weight.data.size(1) * weight.data[0][0].numel()\n",
    "\n",
    "        return weight**16 * math.sqrt(2 / fan_in)\n",
    "\n",
    "    def __call__(self, module,input):\n",
    "        computed_weight = self.compute_weight(module)\n",
    "        setattr(module,'weight',computed_weight)\n",
    "        \n",
    "\n",
    "fn = EqualLR()\n",
    "module = nn.Linear(3,5)\n",
    "module.bias = torch.nn.Parameter(torch.FloatTensor(5).fill_(0))\n",
    "module.register_forward_pre_hook(fn)\n",
    "weight = getattr(module,'weight')\n",
    "del module._parameters['weight']\n",
    "module.register_parameter('weight_orig1', nn.Parameter(weight.data))\n",
    "#setattr(module,'weight_orig2',weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': True,\n",
       " '_parameters': OrderedDict([('bias', Parameter containing:\n",
       "               tensor([0., 0., 0., 0., 0.], requires_grad=True)),\n",
       "              ('weight_orig1',\n",
       "               Parameter containing:\n",
       "               tensor([[ 0.3844,  0.2602, -0.5109],\n",
       "                       [-0.0124, -0.3961,  0.1186],\n",
       "                       [-0.2179, -0.3348, -0.2991],\n",
       "                       [-0.4081, -0.3056,  0.2651],\n",
       "                       [-0.2233, -0.0914, -0.5209]], requires_grad=True))]),\n",
       " '_buffers': OrderedDict(),\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_is_full_backward_hook': None,\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict([(0, <__main__.EqualLR at 0x1a96318a3d0>)]),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_post_hooks': OrderedDict(),\n",
       " '_modules': OrderedDict(),\n",
       " 'in_features': 3,\n",
       " 'out_features': 5}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': True,\n",
       " '_parameters': OrderedDict([('bias', Parameter containing:\n",
       "               tensor([0., 0., 0., 0., 0.], requires_grad=True)),\n",
       "              ('weight_orig1',\n",
       "               Parameter containing:\n",
       "               tensor([[ 0.3844,  0.2602, -0.5109],\n",
       "                       [-0.0124, -0.3961,  0.1186],\n",
       "                       [-0.2179, -0.3348, -0.2991],\n",
       "                       [-0.4081, -0.3056,  0.2651],\n",
       "                       [-0.2233, -0.0914, -0.5209]], requires_grad=True))]),\n",
       " '_buffers': OrderedDict(),\n",
       " '_non_persistent_buffers_set': set(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_is_full_backward_hook': None,\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict([(0, <__main__.EqualLR at 0x1a96318a3d0>)]),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_load_state_dict_post_hooks': OrderedDict(),\n",
       " '_modules': OrderedDict(),\n",
       " 'in_features': 3,\n",
       " 'out_features': 5,\n",
       " 'weight': tensor([[1.8537e-07, 3.5964e-10, 1.7566e-05],\n",
       "         [2.6455e-31, 3.0012e-07, 1.2499e-15],\n",
       "         [2.1159e-11, 2.0347e-08, 3.3430e-09],\n",
       "         [4.8369e-07, 4.7288e-09, 4.8561e-10],\n",
       "         [3.1176e-11, 1.9314e-17, 2.4008e-05]], grad_fn=<MulBackward0>)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy = torch.ones((2,3))\n",
    "module(dummy)\n",
    "module.__dict__\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "styleGAN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1(resl=3):\n",
    "    layer =[]\n",
    "    up = nn.Upsample(scale_factor=2)\n",
    "    if resl>=6:\n",
    "        conv1 = nn.Conv2d(2**(15-resl),2**(14-resl),3,1,1)\n",
    "    else:\n",
    "        conv1 = nn.Conv2d(512,512,3,1,1)\n",
    "    layer.append(up)\n",
    "    layer.append(conv1)\n",
    "\n",
    "    return nn.Sequential(*layer)\n",
    "\n",
    "def conv2(resl=3):\n",
    "    layer =[]\n",
    "    if resl>=6:\n",
    "        conv1 = nn.Conv2d(2**(14-resl),2**(14-resl),3,1,1)\n",
    "    else:\n",
    "        conv1 = nn.Conv2d(512,512,3,1,1)\n",
    "    layer.append(conv1)\n",
    "\n",
    "    return nn.Sequential(*layer)\n",
    "\n",
    "class Noise(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self,x):\n",
    "        batch = x.shape[0]\n",
    "        size = x.size(3)\n",
    "        noise = torch.randn(batch, 1, size, size)\n",
    "        print('noise',noise.shape)\n",
    "        print('noise success')\n",
    "        return x+noise\n",
    "class AdaptiveInstanceNorm(nn.Module):\n",
    "    def __init__(self,ch):\n",
    "        super().__init__()\n",
    "        self.ch = ch\n",
    "        self.instance_norm = nn.InstanceNorm2d(ch)\n",
    "        self.linear = nn.Linear(512,ch*2)\n",
    "    def forward(self,x,style_latent):\n",
    "        batch = x.shape[0]\n",
    "        size = x.size(3)\n",
    "        x_norm = self.instance_norm(x)\n",
    "        ## batch 전부 같은 스타일 넣는 게 맞나??\n",
    "        style_out = self.linear(style_latent) ## style_latent도 batch size 만큼\n",
    "        gamma = style_out[:,:self.ch].unsqueeze(2).unsqueeze(3)\n",
    "        beta = style_out[:,self.ch:].unsqueeze(2).unsqueeze(3)\n",
    "        print('x_norm',x_norm.shape)\n",
    "        print('gamma',gamma.shape)\n",
    "        print('beta',beta.shape)\n",
    "\n",
    "        print('AdaIN success')\n",
    "        sample = gamma*x_norm+beta\n",
    "        print('sample',sample.shape)\n",
    "        return gamma*x_norm+beta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StyleConvBlock만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#StyleConvBlock만들기\n",
    "class StyleConvBlock(nn.Module):\n",
    "    def __init__(self,resl):\n",
    "        super().__init__()\n",
    "        if resl >=6:\n",
    "            ch = 2**(14-resl)\n",
    "        else:\n",
    "            ch= 512\n",
    "        if resl==2:\n",
    "            self.conv1 = nn.Sequential()\n",
    "        else:\n",
    "            self.conv1 = conv1(resl)\n",
    "        self.noise1 = Noise()\n",
    "        self.adain1 = AdaptiveInstanceNorm(ch) #h와 ch를 착각함.\n",
    "        self.conv2 = conv2(resl)\n",
    "        self.noise2 = Noise()\n",
    "        self.adain2 = AdaptiveInstanceNorm(ch)\n",
    "        \n",
    "    def forward(self,x,style_latent): \n",
    "        print('=====styleConvBlock==')\n",
    "        y = self.conv1(x)\n",
    "        y = self.noise1(y)\n",
    "        y = self.adain1(y,style_latent) \n",
    "        y = self.conv2(y)\n",
    "        y = self.noise2(y)\n",
    "        y = self.adain2(y,style_latent)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = torch.ones([2,512,4,4])\n",
    "style_latent = torch.ones([1,512])\n",
    "model = StyleConvBlock(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====styleConvBlock==\n",
      "noise torch.Size([2, 1, 4, 4])\n",
      "noise success\n",
      "x_norm torch.Size([2, 512, 4, 4])\n",
      "gamma torch.Size([1, 512, 1, 1])\n",
      "beta torch.Size([1, 512, 1, 1])\n",
      "AdaIN success\n",
      "sample torch.Size([2, 512, 4, 4])\n",
      "noise torch.Size([2, 1, 4, 4])\n",
      "noise success\n",
      "x_norm torch.Size([2, 512, 4, 4])\n",
      "gamma torch.Size([1, 512, 1, 1])\n",
      "beta torch.Size([1, 512, 1, 1])\n",
      "AdaIN success\n",
      "sample torch.Size([2, 512, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "dummy_output = model(dummy,style_latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = torch.ones([2,512,4,4])\n",
    "style_latent = torch.ones([1,512])\n",
    "model_resl2 = StyleConvBlock(2)\n",
    "model_resl3 = StyleConvBlock(3)\n",
    "model_resl4 = StyleConvBlock(4)\n",
    "model_resl5 = StyleConvBlock(5)\n",
    "model_resl6 = StyleConvBlock(6)\n",
    "model_resl7 = StyleConvBlock(7)\n",
    "model_resl8 = StyleConvBlock(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====styleConvBlock==\n",
      "noise torch.Size([2, 1, 4, 4])\n",
      "noise success\n",
      "x_norm torch.Size([2, 512, 4, 4])\n",
      "gamma torch.Size([1, 512, 1, 1])\n",
      "beta torch.Size([1, 512, 1, 1])\n",
      "AdaIN success\n",
      "sample torch.Size([2, 512, 4, 4])\n",
      "noise torch.Size([2, 1, 4, 4])\n",
      "noise success\n",
      "x_norm torch.Size([2, 512, 4, 4])\n",
      "gamma torch.Size([1, 512, 1, 1])\n",
      "beta torch.Size([1, 512, 1, 1])\n",
      "AdaIN success\n",
      "sample torch.Size([2, 512, 4, 4])\n",
      "=====styleConvBlock==\n",
      "noise torch.Size([2, 1, 8, 8])\n",
      "noise success\n",
      "x_norm torch.Size([2, 512, 8, 8])\n",
      "gamma torch.Size([1, 512, 1, 1])\n",
      "beta torch.Size([1, 512, 1, 1])\n",
      "AdaIN success\n",
      "sample torch.Size([2, 512, 8, 8])\n",
      "noise torch.Size([2, 1, 8, 8])\n",
      "noise success\n",
      "x_norm torch.Size([2, 512, 8, 8])\n",
      "gamma torch.Size([1, 512, 1, 1])\n",
      "beta torch.Size([1, 512, 1, 1])\n",
      "AdaIN success\n",
      "sample torch.Size([2, 512, 8, 8])\n",
      "=====styleConvBlock==\n",
      "noise torch.Size([2, 1, 16, 16])\n",
      "noise success\n",
      "x_norm torch.Size([2, 512, 16, 16])\n",
      "gamma torch.Size([1, 512, 1, 1])\n",
      "beta torch.Size([1, 512, 1, 1])\n",
      "AdaIN success\n",
      "sample torch.Size([2, 512, 16, 16])\n",
      "noise torch.Size([2, 1, 16, 16])\n",
      "noise success\n",
      "x_norm torch.Size([2, 512, 16, 16])\n",
      "gamma torch.Size([1, 512, 1, 1])\n",
      "beta torch.Size([1, 512, 1, 1])\n",
      "AdaIN success\n",
      "sample torch.Size([2, 512, 16, 16])\n",
      "=====styleConvBlock==\n",
      "noise torch.Size([2, 1, 32, 32])\n",
      "noise success\n",
      "x_norm torch.Size([2, 512, 32, 32])\n",
      "gamma torch.Size([1, 512, 1, 1])\n",
      "beta torch.Size([1, 512, 1, 1])\n",
      "AdaIN success\n",
      "sample torch.Size([2, 512, 32, 32])\n",
      "noise torch.Size([2, 1, 32, 32])\n",
      "noise success\n",
      "x_norm torch.Size([2, 512, 32, 32])\n",
      "gamma torch.Size([1, 512, 1, 1])\n",
      "beta torch.Size([1, 512, 1, 1])\n",
      "AdaIN success\n",
      "sample torch.Size([2, 512, 32, 32])\n",
      "=====styleConvBlock==\n",
      "noise torch.Size([2, 1, 64, 64])\n",
      "noise success\n",
      "x_norm torch.Size([2, 256, 64, 64])\n",
      "gamma torch.Size([1, 256, 1, 1])\n",
      "beta torch.Size([1, 256, 1, 1])\n",
      "AdaIN success\n",
      "sample torch.Size([2, 256, 64, 64])\n",
      "noise torch.Size([2, 1, 64, 64])\n",
      "noise success\n",
      "x_norm torch.Size([2, 256, 64, 64])\n",
      "gamma torch.Size([1, 256, 1, 1])\n",
      "beta torch.Size([1, 256, 1, 1])\n",
      "AdaIN success\n",
      "sample torch.Size([2, 256, 64, 64])\n",
      "=====styleConvBlock==\n",
      "noise torch.Size([2, 1, 128, 128])\n",
      "noise success\n",
      "x_norm torch.Size([2, 128, 128, 128])\n",
      "gamma torch.Size([1, 128, 1, 1])\n",
      "beta torch.Size([1, 128, 1, 1])\n",
      "AdaIN success\n",
      "sample torch.Size([2, 128, 128, 128])\n",
      "noise torch.Size([2, 1, 128, 128])\n",
      "noise success\n",
      "x_norm torch.Size([2, 128, 128, 128])\n",
      "gamma torch.Size([1, 128, 1, 1])\n",
      "beta torch.Size([1, 128, 1, 1])\n",
      "AdaIN success\n",
      "sample torch.Size([2, 128, 128, 128])\n",
      "=====styleConvBlock==\n",
      "noise torch.Size([2, 1, 256, 256])\n",
      "noise success\n",
      "x_norm torch.Size([2, 64, 256, 256])\n",
      "gamma torch.Size([1, 64, 1, 1])\n",
      "beta torch.Size([1, 64, 1, 1])\n",
      "AdaIN success\n",
      "sample torch.Size([2, 64, 256, 256])\n",
      "noise torch.Size([2, 1, 256, 256])\n",
      "noise success\n",
      "x_norm torch.Size([2, 64, 256, 256])\n",
      "gamma torch.Size([1, 64, 1, 1])\n",
      "beta torch.Size([1, 64, 1, 1])\n",
      "AdaIN success\n",
      "sample torch.Size([2, 64, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dummy_output = model_resl2(dummy,style_latent)\n",
    "dummy_output = model_resl3(dummy_output,style_latent)\n",
    "dummy_output = model_resl4(dummy_output,style_latent)\n",
    "dummy_output = model_resl5(dummy_output,style_latent)\n",
    "dummy_output = model_resl6(dummy_output,style_latent)\n",
    "dummy_output = model_resl7(dummy_output,style_latent)\n",
    "dummy_output = model_resl8(dummy_output,style_latent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StyleConvBlock(\n",
      "  (conv1): Sequential(\n",
      "    (0): Upsample(scale_factor=2.0, mode=nearest)\n",
      "    (1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (noise1): Noise()\n",
      "  (adain1): AdaptiveInstanceNorm(\n",
      "    (instance_norm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (linear): Linear(in_features=512, out_features=128, bias=True)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (noise2): Noise()\n",
      "  (adain2): AdaptiveInstanceNorm(\n",
      "    (instance_norm): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (linear): Linear(in_features=512, out_features=128, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_resl8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_rgb_layer(resl=3):\n",
    "    if resl>=6:\n",
    "        ch = 2**(14-resl)\n",
    "        to_rgb = nn.Conv2d(ch,3,3,1,1)\n",
    "    else:\n",
    "        to_rgb = nn.Conv2d(512,3,3,1,1)\n",
    "    return to_rgb\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====ToRgbBlock==\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 256, 256])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_rgb = ToRgbBlock(8)\n",
    "to_rgb(dummy_output,style_latent).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_conv_blocks = [\n",
    "    StyleConvBlock(resl=2),\n",
    "    StyleConvBlock(resl=3),\n",
    "    StyleConvBlock(resl=4),\n",
    "    StyleConvBlock(resl=5),\n",
    "    StyleConvBlock(resl=6),\n",
    "    StyleConvBlock(resl=7),\n",
    "    StyleConvBlock(resl=8),\n",
    "]\n",
    "to_rgb_layers = [\n",
    "    to_rgb_layer(resl=2),\n",
    "    to_rgb_layer(resl=3),\n",
    "    to_rgb_layer(resl=4),\n",
    "    to_rgb_layer(resl=5),\n",
    "    to_rgb_layer(resl=6),\n",
    "    to_rgb_layer(resl=7),\n",
    "    to_rgb_layer(resl=8),\n",
    "]\n",
    "layers =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mySequential(nn.Sequential):\n",
    "    def forward(self, *inputs):\n",
    "        for module in self._modules.values():\n",
    "            if type(inputs) == tuple:\n",
    "                inputs = module(*inputs)\n",
    "            else:\n",
    "                inputs = module(inputs)\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_num = 1\n",
    "for i in range(iter_num):\n",
    "    layers.append(style_conv_blocks[i])\n",
    "layers.append(to_rgb_layers[iter_num-1])\n",
    "#mymodel = nn.Sequential(*layers)\n",
    "mymodel = mySequential()\n",
    "mymodel.add_module('asdf',style_conv_blocks[0])\n",
    "mymodel.add_module('qwer',to_rgb_layers[0])\n",
    "\n",
    "\n",
    "# mymodel2 = nn.Sequential()\n",
    "# linear1 = nn.Linear(512,128)\n",
    "# linear2 = nn.Linear(128,8)\n",
    "# mymodel2.add_module('(0)',linear1)\n",
    "# mymodel2.add_module('(1)',linear2)\n",
    "# linear1.add_module('(1)',linear2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mySequential(\n",
      "  (asdf): StyleConvBlock(\n",
      "    (conv1): Sequential()\n",
      "    (noise1): Noise()\n",
      "    (adain1): AdaptiveInstanceNorm(\n",
      "      (instance_norm): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      (linear): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    )\n",
      "    (conv2): Sequential(\n",
      "      (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (noise2): Noise()\n",
      "    (adain2): AdaptiveInstanceNorm(\n",
      "      (instance_norm): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      (linear): Linear(in_features=512, out_features=1024, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (qwer): Conv2d(512, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(mymodel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StyleConvBlock(\n",
      "  (conv1): Sequential()\n",
      "  (noise1): Noise()\n",
      "  (adain1): AdaptiveInstanceNorm(\n",
      "    (instance_norm): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (linear): Linear(in_features=512, out_features=1024, bias=True)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (noise2): Noise()\n",
      "  (adain2): AdaptiveInstanceNorm(\n",
      "    (instance_norm): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (linear): Linear(in_features=512, out_features=1024, bias=True)\n",
      "  )\n",
      ")\n",
      "-------------------\n",
      "=====styleConvBlock==\n",
      "noise torch.Size([2, 1, 4, 4])\n",
      "noise success\n",
      "x_norm torch.Size([2, 512, 4, 4])\n",
      "gamma torch.Size([1, 512, 1, 1])\n",
      "beta torch.Size([1, 512, 1, 1])\n",
      "AdaIN success\n",
      "sample torch.Size([2, 512, 4, 4])\n",
      "noise torch.Size([2, 1, 4, 4])\n",
      "noise success\n",
      "x_norm torch.Size([2, 512, 4, 4])\n",
      "gamma torch.Size([1, 512, 1, 1])\n",
      "beta torch.Size([1, 512, 1, 1])\n",
      "AdaIN success\n",
      "sample torch.Size([2, 512, 4, 4])\n",
      "Conv2d(512, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "-------------------\n",
      "============ torch.Size([2, 3, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "dummy = torch.ones([2,512,4,4])\n",
    "style_latent = torch.ones([1,512])\n",
    "dummy_output = mymodel(dummy,style_latent)\n",
    "print(\"============\",dummy_output.shape)\n",
    "# mymodel2(dummy_output,style_latent).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## block 연결해서 완성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Style Conv Block 을 만들고 연결.\n",
    "# to_rgb layer 만들고 연결\n",
    "# to_rgb layer는 상황에 따라 값이 바뀜\n",
    "\n",
    "class SynthesisGen(nn.Module):\n",
    "    def __init__(self,resl):\n",
    "        super().__init__()\n",
    "        self.style_conv_blocks = [\n",
    "            StyleConvBlock(resl=2),\n",
    "            StyleConvBlock(resl=3),\n",
    "            StyleConvBlock(resl=4),\n",
    "            StyleConvBlock(resl=5),\n",
    "            StyleConvBlock(resl=6),\n",
    "            StyleConvBlock(resl=7),\n",
    "            StyleConvBlock(resl=8),\n",
    "        ]\n",
    "        self.to_rgb_layers = [\n",
    "            to_rgb_layer(resl=2),\n",
    "            to_rgb_layer(resl=3),\n",
    "            to_rgb_layer(resl=4),\n",
    "            to_rgb_layer(resl=5),\n",
    "            to_rgb_layer(resl=6),\n",
    "            to_rgb_layer(resl=7),\n",
    "            to_rgb_layer(resl=8),\n",
    "        ]\n",
    "        self.synthesis_net = self.set_synthesis_network(resl)\n",
    "    def set_synthesis_network(self,resl=2):\n",
    "        # st_conv resl-1개수만큼 연결 <- for 문\n",
    "        # [resl-1] 인덱싱으로 to_rgb 붙이기\n",
    "        iter_num = resl-1\n",
    "        layers=[]\n",
    "        # for i in range(iter_num):\n",
    "        #     layers.append(self.style_conv_blocks[i])\n",
    "        layers.append(self.style_conv_blocks[0])\n",
    "        layers.append(self.to_rgb_layers[iter_num-1])\n",
    "        return mySequential(*layers)\n",
    "\n",
    "    def forward(self,x,style_latent_w):\n",
    "        to_rgb=self.synthesis_net(x,style_latent_w) ##여기가 error\n",
    "        return to_rgb\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_input = torch.randn((2,512,4,4)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class styleFC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,512),\n",
    "            nn.ReLU(), # 마지막 activation fn은 뭐지?\n",
    "        )\n",
    "    def forward(self,x): \n",
    "        #x [1,512]\n",
    "        return self.fc(x) #[1,512]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class styleGen(nn.Module):\n",
    "    def __init__(self,resl=3):\n",
    "        super().__init__()\n",
    "        self.style_fc = styleFC()\n",
    "        self.synthesis_gen = SynthesisGen(resl)\n",
    "        \n",
    "# style latent W 만들고 연결\n",
    "\n",
    "    def forward(self,z,constant_input):\n",
    "        style_latent_w = self.style_fc(z)\n",
    "        output = self.synthesis_gen(constant_input,style_latent_w)\n",
    "        #그럼 to rgb 거쳐서 이미지로 나오겠지.\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_style_gen = styleGen(3)\n",
    "constant_input_dummy = torch.ones((2,512,4,4))\n",
    "z = torch.ones((1,512))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1292,  0.4887, -1.8443],\n",
       "        [ 0.2806, -0.4141, -0.2033]], requires_grad=True)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "du = torch.randn(2,3)\n",
    "\n",
    "print(du.shape)\n",
    "du.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.]],\n",
       "\n",
       "         [[0.]],\n",
       "\n",
       "         [[0.]]]], requires_grad=True)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dudu =nn.Parameter(torch.zeros(1, 3, 1, 1))\n",
    "dudu =torch.zeros(1, 3, 1, 1)\n",
    "print(dudu.shape)\n",
    "dudu.requires_grad_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "747fe91b7bc9ec9d8624ac8c139c41948fb906c2c40d5ffbc4d71da454373257"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
